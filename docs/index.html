<!DOCTYPE html>
<html>
<head>
<head>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/@picocss/pico@2/css/pico.classless.min.css"
>
</head>
<body>
<header>
<nav>
  <ul>
    <li><a href="/"><strong>HEROP Geodata</strong></a></li>
  </ul>
  <ul>
    <li><a href="/#specs">Specs</a></li>
    <li><a href="/#cli">CLI</a></li>
    <li><a href="/downloads.html">Downloads &rarr;</a></li>
  </ul>
</nav>
</header>
<main>
<h1 id="herop-geodata">HeRoP Geodata</h1>
<p>This website provides download links for geospatial datasets that we at the <a href="https://healthyregions.org">Healthy Regions &amp; Policies Lab</a> use for various analyses and cartographic applications. The <a href="https://github.com/healthyregions/geodata">source repository</a> holds the Python processing pipeline that we use to generate these datasets directly from the US Census Bureau <a href="https://www2.census.gov/geo">FTP</a>.</p>
<ul>
<li><a href="#processing-steps">Processing steps</a></li>
<li><a href="#specs">File specs</a></li>
<li><a href="#cli">CLI</a></li>
<li><a href="#building-the-website">Building the website</a></li>
<li><a href="https://github.com/healthyregions/geodata/blob/main/uploads-list.csv">Download list CSV</a></li>
</ul>
<h2 id="processing-steps">Processing Steps</h2>
<p>Given a geography, year, and scale, all matching files in the Census FTP will be processed accordingly:</p>
<ol>
<li>Downloaded and unzipped within <code>.cache/</code></li>
<li>Loaded and merged into a single geopandas dataframe</li>
<li>New fields calculated and added to the data frame</li>
<li>The data frame is exported to one or more of the following file formats: Shapefile, GeoJSON, and PMTiles</li>
</ol>
<h2 id="specs">Specs</h2>
<p>The output files from this process have a few extra fields added to them, and come in a few different file formats that are useful for different contexts.</p>
<h3 id="fields">Fields</h3>
<details name="fields">
  <summary role="button"><strong>LABEL</strong></summary>
    A human readable label is calculated using each unit's name and proper LSAD.
</details>
<details name="fields">
  <summary role="button"><strong>BBOX</strong></summary>
    The bounding box of each feature is calculated and concatenated into a single text field with this format: "{minx},{miny},{maxx},{maxy}".
</details>
<details name="fields">
  <summary role="button"><strong>HEROP_ID</strong></summary>
<p>In some of our projects we use what we call a <strong>HEROP_ID</strong> to identify geographic boundaries defined by the US Census Bureau, which is a slight variation on the commonly used standard <strong>GEOID</strong>. Our format is similar to what the American FactFinder used (now data.census.gov). </p>
<p>A HEROP_ID consists of three parts:</p>
<ol>
<li>
The 3-digit <a href="https://www.census.gov/programs-surveys/geography/technical-documentation/naming-convention/cartographic-boundary-file/carto-boundary-summary-level.html">Summary Level Code</a> for this geography. Common summary level codes are:<ul>
<li><code>040</code> -- <strong>State</strong></li>
<li><code>050</code> -- <strong>County</strong></li>
<li><code>140</code> -- <strong>Census Tract</strong></li>
<li><code>150</code> -- <strong>Census Block Group</strong></li>
<li><code>860</code> -- <strong>Zip Code Tabulation Area (ZCTA)</strong></li>
</ul>
</li>
<li>The 2-letter string <code>US</code></li>
<li>
The standard <a href="https://www.census.gov/programs-surveys/geography/guidance/geo-identifiers.html">GEOID</a> for the given unit (length depends on unit summary)<ul>
<li>GEOIDs are, in turn, hierarchical aggregations of FIPS codes</li>
</ul>
</li>
</ol>
<p>Expanding out the FIPS codes for the five summary levels shown above, the full IDs would look like:</p>
<table>
<thead>
<tr>
<th>summary level</th>
<th>format</th>
<th>length</th>
<th>example</th>
</tr>
</thead>
<tbody>
<tr>
<td>State</td>
<td><code>040US</code> + <code>STATE (2)</code></td>
<td>7</td>
<td><code>040US17</code> (Illinois)</td>
</tr>
<tr>
<td>County</td>
<td><code>050US</code> + <code>STATE (2)</code> + <code>COUNTY (3)</code></td>
<td>10</td>
<td><code>050US17019</code> (Champaign County)</td>
</tr>
<tr>
<td>Tract</td>
<td><code>140US</code> + <code>STATE (2)</code> + <code>COUNTY (3)</code> + <code>TRACT (6)</code></td>
<td>16</td>
<td><code>140US17019005900</code></td>
</tr>
<tr>
<td>Block Group</td>
<td><code>150US</code> + <code>STATE (2)</code> + <code>COUNTY (3)</code> + <code>TRACT (6)</code> + <code>BLOCK GROUP (1)</code></td>
<td>17</td>
<td><code>150US170190059002</code></td>
</tr>
<tr>
<td>ZCTA</td>
<td><code>860US</code> + <code>ZIP CODE (5)</code></td>
<td>10</td>
<td><code>860US61801</code></td>
</tr>
</tbody></table><p>The advantages of this composite ID are:</p>
<ol>
<li>Unique across all geographic areas in the US</li>
<li>Will always be forced to string formatting</li>
<li>Easy to programmatically change back into the more standard GEOIDs</li>
</ol>
<p><strong>Convert to GEOID (integers)</strong></p>
<p>The <code>HEROP_ID</code> can be converted back to standard GEOIDs by removing the first 5 characters, or by taking everything after the substring &quot;US&quot;. Here are some examples of what this looks like in different software:</p>
<ul>
<li>Excel: <code>REPLACE(A1, 1, 5, &quot;&quot;)</code></li>
<li>R: <code>geoid &lt;- str_split_i(HEROP_ID, &quot;US&quot;, -1)</code></li>
<li>Python: <code>geoid = HEROP_ID.split(&quot;US&quot;)[1]</code></li>
<li>JavaScript: <code>const geoid = HEROP_ID.split(&quot;US&quot;)[1]</code></li>
</ul>
</details>
<h3 id="formats">Formats</h3>
<p>Each processed dataset is exported to three different file formats.</p>
<details name="formats">
  <summary role="button"><strong>GeoJSON</strong></summary>
    A simple plain text format that is good for small to medium size datasets and can be used in a wide variety of web and desktop software [learn more](https://geojson.org/)
</details>
<details name="formats">
  <summary role="button"><strong>PMTiles</strong></summary>
    A "cloud-native" vector format that is very fast in the right web mapping environment [learn more](https://docs.protomaps.com/pmtiles/)
</details>
<details name="formats">
  <summary role="button"><strong>Shapefile</strong></summary>
    Used in scripting and desktop software for performant display and analysis [learn more](https://www.geographyrealm.com/what-is-a-shapefile/)
</details>
<h4 id="using-shapefiles-in-scripts">Using Shapefiles in scripts</h4>
<p>You don't need to download and unzip these shapefiles to use them in R or Python scripts.</p>
<ul>
<li>
<p><strong>R Example</strong>: <code>sf</code> allows you to directly open remote, zipped shapefiles without downloading them <a href="https://r-spatial.github.io/sf">learn more, <code>read_sf</code> seems not to be documented though (?)</a>:</p>
<pre><code>library(&#x27;sf&#x27;)
tracts &lt;- read_sf(&#x27;/vsizip//vsicurl/https://herop-geodata.s3.us-east-2.amazonaws.com/oeps/tract-2018-500k-shp.zip&#x27;)
</code></pre>
</li>
<li>
<p><strong>Python Example</strong>: <code>geopandas</code> allows you to directly open remote, zipped shapefiles files without downloading them <a href="https://geopandas.org/en/stable/docs/reference/api/geopandas.read_file.html">learn more</a>:</p>
<pre><code>import geopandas as gpd
tracts = gpd.read_file(&quot;/vsizip//vsicurl/https://herop-geodata.s3.us-east-2.amazonaws.com/oeps/state-2010-500k-shp.zip&quot;)
</code></pre>
</li>
</ul>
<h2 id="cli">CLI</h2>
<p>The script we use for this processing pipeline can be run on its own to generate new copies of the files. This section serves to help with development and maintance of that script.</p>
<h3 id="install">Install</h3>
<pre><code>git clone https://github.com/healthyregions/census-geofiles
cd census-geofiles
python3 -m venv env &amp;&amp; source ./env/bin/activate
pip install -e .
</code></pre>
<p>To create exports in PMTiles format, you must also install <a href="https://github.com/felt/tippecanoe?tab=readme-ov-file#installation">tippecanoe</a> and provide the path to its executable through an environment variable or command line argument (see below)</p>
<h2 id="configuration">Configuration</h2>
<p>A few environment variables should be set before running the command.</p>
<pre><code>cp .env.example .env
</code></pre>
<p>Our defaults are provided in the example file along with comments on each variable. These variables can be overwritten at runtime like so:</p>
<pre><code>AWS_BUCKET_NAME=my-bucket python ./census.py etc...
</code></pre>
<h3 id="available-mirrors">Available mirrors</h3>
<p>By default, the process will download files directly from the US Census FTP, <a href="https://www2.census.gov/geo">https://www2.census.gov/geo</a>. You can direct it to use mirrors of that FTP if needed.</p>
<table>
<thead>
<tr>
<th>Institution</th>
<th>Link</th>
<th><code>MIRROR_URL</code></th>
</tr>
</thead>
<tbody>
<tr>
<td>University of Chicago</td>
<td><a href="https://datamirror.lib.uchicago.edu/census-tiger">browse</a></td>
<td><code>https://pub-a835f667d17f4b6691fafec7e9ede33d.r2.dev</code></td>
</tr>
</tbody></table><h3 id="sources-lookup">Sources lookup</h3>
<p>The file <a href="./lookups/sources.json">lookups/sources.json</a> is a master list of all file urls and important field names for each year, geography, and scale. This is necessary because field names and file naming conventions have changed over the years (and I have had trouble running <code>ls</code>-type commands on the Census FTP server so for now this config is all just hard-coded).</p>
<h2 id="usage">Usage</h2>
<pre><code>python ./census.py [OPTIONS]

</code></pre>
<p>Options:</p>
<table>
<thead>
<tr>
<th>Arg</th>
<th>Input</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>-g/--geography</td>
<td>place,bg,tract,state,zcta,county</td>
<td>Specify a geography to prepare. If left empty, all geographies will be processed.</td>
</tr>
<tr>
<td>-y/--year</td>
<td>2010,2018,2020</td>
<td>Specify one or more years. If left empty, all years will be processed.</td>
</tr>
<tr>
<td>-s/--scale</td>
<td>500k,tiger</td>
<td>Specify one or more scales of geographic boundary file. If left empty, all scales will be processed.</td>
</tr>
<tr>
<td>--destination</td>
<td>path/to/directory</td>
<td>Output directory for export. If not provided, results will be in .cache/{{geography}}/processed.</td>
</tr>
<tr>
<td>--upload</td>
<td>(flag)</td>
<td>Upload the processed files to S3. Bucket name, AWS creds, and prefix will be acquired from environment variables.</td>
</tr>
<tr>
<td>--no-cache</td>
<td>(flag)</td>
<td>Force re-retrieval of source files.</td>
</tr>
<tr>
<td>--verbose</td>
<td>(flag)</td>
<td>Enable verbose output during process.</td>
</tr>
</tbody></table><p>Note: To export PMTiles, you will need to instl
The available options for geography, year, and scale are collected from <code>sources.json</code> which will continue to expand, so the best way to see all options is by running:</p>
<pre><code>python ./census.py --help
</code></pre>
<p>If no arguments are provided, the entire <code>sources</code> lookup will be traversed and an attempt will be made to generate each file format (Shapefile, GeoJSON, and PMTiles) for every configured year, geography, and scale.</p>
<h3 id="examples">Examples</h3>
<pre><code>python ./census.py -y 2020 -g state -s 500k -f geojson --destination .
</code></pre>
<p>Result: A new GeoJSON file in the local directory, from the 500k Cartographic Boundary shapefile, 2020 vintage.</p>
<pre><code>python ./census.py -g bg -s 500k -f pmtiles --upload
</code></pre>
<p>Result: 500k scale cartographic boundary files for block groups will be merged into nation-wide coverage and exported to PMTiles, one file per available year. Each file will be uploaded to the S3 bucket as described via environment variables.</p>
<h2 id="building-the-website">Building the website</h2>
<p>A separate script <code>build_pages.py</code> script generates the single HTML file hosted on Github pages. After the package has been installed as above, run <code>python ./build_pages.py</code>. The <code>docs/index.html</code> file will be re-rendered based on the main README.</p>

</main>
</body>